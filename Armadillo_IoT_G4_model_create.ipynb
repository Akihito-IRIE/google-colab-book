{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Armadillo-IoT_G4_model_create.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akihito-IRIE/google-colab-book/blob/main/Armadillo_IoT_G4_model_create.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# はじめに\n",
        "\n",
        "本ドキュメントではArmadillo-IoT ゲートウェイ G4上で使用できるTFLite形式の推論モデルを、既存のモデルをベースに学習する転移学習を行い作成するサンプルを紹介します。\n",
        "\n",
        "内容はほぼTensorFlowのObject Detection APIのサンプルそのままですが、最後にSavedModelをTFLite形式に変換する手順を追加しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U2sv0upw04O"
      },
      "source": [
        "# セットアップ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0rKBV4uZacD"
      },
      "source": [
        "!pip install -U --pre tensorflow==\"2.7.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# 存在しなければtensoorflowのmodelsリポジトリをクローンします\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "source": [
        "# Object Detection APIをインストールします\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install . --use-deprecated=legacy-resolver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZcqD4NLdnf4"
      },
      "source": [
        "# 以後の作業に必要なライブラリをimportします\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import imageio\n",
        "import glob\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "##ユーティリティ\n",
        "\n",
        "以後の作業に必要な関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: a file path.\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "  \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "  Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "      and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "      this function assumes that the boxes to be plotted are groundtruth\n",
        "      boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "      category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "  \"\"\"\n",
        "  image_np_with_annotations = image_np.copy()\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_annotations,\n",
        "      boxes,\n",
        "      classes,\n",
        "      scores,\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      min_score_thresh=0.6)\n",
        "  if image_name:\n",
        "    plt.imsave(image_name, image_np_with_annotations)\n",
        "  else:\n",
        "    plt.imshow(image_np_with_annotations)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaXL28TZfk1"
      },
      "source": [
        "## 教師データの準備\n",
        "\n",
        "転移学習をするために必要な、教師データを作成します。\n",
        "\n",
        "物体検出において、以下の2つを合わせたデータを教師データとしています。\n",
        "* 認識させたい物が写った画像群\n",
        "* 画像のどこに物があるかを表すアノテーションデータ\n",
        "\n",
        "そのため、まずは画像データを収集します。\n",
        "\n",
        "今回のサンプルアプリケーションでは、TensorFlow公式の[githubリポジトリ内の画像](https://github.com/tensorflow/models/tree/master/research/object_detection/test_images/ducky/train)(ゴム製のアヒルのおもちゃ)を使用します。\n",
        "\n",
        "なお、本手順で使用する訓練済み物体検出推論モデル元となった[COCOデータセット](https://cocodataset.org/#explore)には、ラバーダックも動物としてのアヒルも含まれていません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQy3ND7EpFQM"
      },
      "source": [
        "# 画像データをロードし、表示します\n",
        "train_image_dir = 'models/research/object_detection/test_images/ducky/train/'\n",
        "train_images_np = []\n",
        "for i in range(1, 6):\n",
        "  image_path = os.path.join(train_image_dir, 'robertducky' + str(i) + '.jpg')\n",
        "  train_images_np.append(load_image_into_numpy_array(image_path))\n",
        "\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "  plt.subplot(2, 3, idx+1)\n",
        "  plt.imshow(train_image_np)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqb_yjAo3cO_"
      },
      "source": [
        "## データのアノテーション\n",
        "\n",
        "次に、画像のどこに何が写っているかを表すアノテーションデータを作成します。\n",
        "\n",
        "TODO: VoTTなどのアノテーションツールの使い方についてまとめる\n",
        "\n",
        "```\n",
        "colab_utils.annotate\n",
        "```\n",
        "を用いることで、Google Colaboratory上でもアノテーションを行うことができます。\n",
        "\n",
        "今回は事前にアノテーションしたバウンディングボックスを用いて教師データを作成します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVS0t0rm76zk"
      },
      "source": [
        "# 以下の2行をアンコメントすることで、Googlel Colaboratory上でも画像にアノテーションを行うことができます\n",
        "# gt_boxes = []\n",
        "# colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)\n",
        "\n",
        "# 上2行の方法でアノテーションを行う場合は以下をコメントアウトしてください\n",
        "gt_boxes = [\n",
        "            np.array([[0.436, 0.591, 0.629, 0.712]], dtype=np.float32),\n",
        "            np.array([[0.539, 0.583, 0.73, 0.71]], dtype=np.float32),\n",
        "            np.array([[0.464, 0.414, 0.626, 0.548]], dtype=np.float32),\n",
        "            np.array([[0.313, 0.308, 0.648, 0.526]], dtype=np.float32),\n",
        "            np.array([[0.256, 0.444, 0.484, 0.629]], dtype=np.float32)\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MitnBFe784fV"
      },
      "source": [
        "次に、クラスのアノテーションを追加します。\n",
        "このサンプルではわかりやすくするために、単一の「rubber_dukey」クラスを指定していますが、複数のクラスを扱うように拡張することも可能です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m94StNoE9Ma3"
      },
      "source": [
        "# 慣習的にクラスIDは1から数え始めます\n",
        "duck_class_id = 1\n",
        "num_classes = 1\n",
        "\n",
        "category_index = {duck_class_id: {'id': duck_class_id, 'name': 'rubber_ducky'}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ohLJeBr9PdA"
      },
      "source": [
        "入力データ(教師データや画像)をトレーニンググループが期待するフォーマットに変換します。  \n",
        "期待するフォーマットはモデルによって異なります(tensor型やワンショット表現など)。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIAT6ZUmdHOC"
      },
      "source": [
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "for (train_image_np, gt_box_np) in zip(\n",
        "    train_images_np, gt_boxes):\n",
        "  train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "      train_image_np, dtype=tf.float32), axis=0))\n",
        "  gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "  zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
        "      np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "  gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "      zero_indexed_groundtruth_classes, num_classes))\n",
        "print('データの事前準備が完了しました。')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3_Z3mJWN9KJ"
      },
      "source": [
        "ラバーダックを正しくアノテーションできたかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBD6l-E4N71y"
      },
      "source": [
        "dummy_scores = np.array([1.0], dtype=np.float32)  # ダミーのスコアとして100%(=1.0)に設定します。\n",
        "\n",
        "plt.figure(figsize=(30, 15))\n",
        "for idx in range(5):\n",
        "  plt.subplot(2, 3, idx+1)\n",
        "  plot_detections(\n",
        "      train_images_np[idx],\n",
        "      gt_boxes[idx],\n",
        "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "      dummy_scores, category_index)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghDAsqfoZvPh"
      },
      "source": [
        "## モバイル端末向け学習済みモデルのロード\n",
        "\n",
        "ここでは、モバイル端末向けの物体検出モデルのアーキテクチャである「SSD MobileNet V2 FPN-Lite」をビルドし、最上位の分類層を除く全ての層を復元します。\n",
        "\n",
        "**執筆時点では、最終的に出力するTFLite形式の物体検出推論モデルは、SSDモデル以外サポートしていないことに注意してください。**\n",
        "\n",
        "本サンプルでは、今回使用するSSDアーキテクチャに合わせていくつかの項目をハードコーディングしています(入力画像サイズが常に320x320であることを想定しているなど)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J16r3NChD-7"
      },
      "source": [
        "# checkpointファイルをダウンロードしてmodels/research/object_detection/test_data/に移動させます。\n",
        "\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "!if [ -d \"models/research/object_detection/test_data/checkpoint\" ]; then rm -Rf models/research/object_detection/test_data/checkpoint; fi\n",
        "!mkdir models/research/object_detection/test_data/checkpoint\n",
        "!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyT4BUbaMeG-"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print('モデルをビルドし、ファインチューニングの為に重みパラメータを復元します。', flush=True)\n",
        "num_classes = 1\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config'\n",
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# 後でTFLiteファイルを作成するために、outpuy_directoryにcheckpointファイルとconfigファイルを保存します。\n",
        "output_directory = 'output/'\n",
        "output_checkpoint_dir = os.path.join(output_directory, 'checkpoint')\n",
        "\n",
        "# パイプラインコンフィグをロードし、物体検出モデルをビルドします。\n",
        "#\n",
        "# 90個のクラスを予測するCOCOアーキテクチャを使用しているので、\n",
        "# num_classフィールドを上書きして1個の新しいクラス(rubber_ducky)を作成します。\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "model_config.ssd.num_classes = num_classes\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)\n",
        "# 新しいパイプラインコンフィグを保存します。\n",
        "pipeline_proto = config_util.create_pipeline_proto_from_configs(configs)\n",
        "config_util.save_pipeline_config(pipeline_proto, output_directory)\n",
        "\n",
        "# オブジェクトベースのチェックポイントリストアの設定 \n",
        "# SSDには2つの予測ヘッドがあります。\n",
        "# 1つは分類用、もう1つはボックス回帰用です。 \n",
        "# ここではボックス回帰ヘッドを復元し、分類ヘッドを最初から初期化します。\n",
        "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    # 以下の行をアンコメントすることで、両方のヘッドを復元することができます。\n",
        "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "fake_model = tf.compat.v2.train.Checkpoint(\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          _box_predictor=fake_box_predictor)\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
        "ckpt.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "# TFLiteへの変換のために、checkpointファイルを保存します。\n",
        "exported_ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt_manager = tf.train.CheckpointManager(\n",
        "    exported_ckpt, output_checkpoint_dir, max_to_keep=1)\n",
        "\n",
        "# ダミーの画像を使用して推論を実行します。\n",
        "image, shapes = detection_model.preprocess(tf.zeros([1, 320, 320, 3]))\n",
        "prediction_dict = detection_model.predict(image, shapes)\n",
        "_ = detection_model.postprocess(prediction_dict, shapes)\n",
        "print('重みパラメータを復元しました。')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCkWmdoZZ0zJ"
      },
      "source": [
        "## 転移学習\n",
        "\n",
        "ロードした学習済みモデルに対して、用意した教師データを使って転移学習を行います。\n",
        "\n",
        "この手順でのいくつかのパラメータは実験的に設定されています。\n",
        "例えばSGDの場合、\"learning rate\"、\"num_batches\"、\"momentum\"パラメータなどです。\n",
        "\n",
        "これらはあくまでもサンプルのパラメータ設定であり、最良の結果を得るためには、データやモデルのアーキテクチャに合わせてチューニングする必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyHoF4mUrv5-"
      },
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# これらのパラメータは チューニング可能です。\n",
        "# 今回の例では学習用の画像は5枚なので、これ以上バッチサイズを大きくする意味はありませんが、\n",
        "# 必要であれば設定可能です。\n",
        "batch_size = 5\n",
        "learning_rate = 0.15\n",
        "num_batches = 1000\n",
        "\n",
        "# 最上位のレイヤーのファインチューニングする変数を選択します。\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "to_fine_tune = []\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "for var in trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)\n",
        "\n",
        "# 学習のために、前後方のパスを設定します。\n",
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "\n",
        "  # 少しだけ実行速度が速くなるため、tf.functionデコレーターを使用しています。\n",
        "  # \"@tf.function\"をコメントアウトすることで、中でどのような処理が行われているか見ることができます。\n",
        "  @tf.function\n",
        "  def train_step_fn(image_tensors,\n",
        "                    groundtruth_boxes_list,\n",
        "                    groundtruth_classes_list):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 320x320.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "    shapes = tf.constant(batch_size * [[320, 320, 3]], dtype=tf.int32)\n",
        "    model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list)\n",
        "    with tf.GradientTape() as tape:\n",
        "      preprocessed_images = tf.concat(\n",
        "          [detection_model.preprocess(image_tensor)[0]\n",
        "           for image_tensor in image_tensors], axis=0)\n",
        "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
        "      losses_dict = model.loss(prediction_dict, shapes)\n",
        "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "    return total_loss\n",
        "\n",
        "  return train_step_fn\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "train_step_fn = get_model_train_step_function(\n",
        "    detection_model, optimizer, to_fine_tune)\n",
        "\n",
        "print('ファインチューニングを開始します。', flush=True)\n",
        "for idx in range(num_batches):\n",
        "  # 教師データとなる画像をランダムに取得\n",
        "  all_keys = list(range(len(train_images_np)))\n",
        "  random.shuffle(all_keys)\n",
        "  example_keys = all_keys[:batch_size]\n",
        "\n",
        "  # なお、このデモではデータのかさ増しはしていません。\n",
        "  # ランダムに画像を回転させたり、切り抜いたりしたものを教師データとすることで\n",
        "  # より精度が高いモデルが作成できることがあります。\n",
        "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "  # トレーニングを行います。\n",
        "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "\n",
        "  if idx % 100 == 0:\n",
        "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "\n",
        "print('ファインチューニングが完了しました。')\n",
        "\n",
        "ckpt_manager.save()\n",
        "print('checkpointファイルを保存しました。')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYk1_9Fc2lZO"
      },
      "source": [
        "# TensorFlow Lite形式にエクスポート&実行\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0nsDVEd9SuX"
      },
      "source": [
        "## モデルの変換\n",
        "\n",
        "上記手順で作成した推論モデルはTensorFlow形式ではありますが、Armadillo-IoT ゲートウェイ G4では使用できない形式です。\n",
        "\n",
        "モバイル端末向けに軽量化されたTFLite形式に変換しなければなりませんので、以下に手順を紹介します。\n",
        "\n",
        "最初に、`export_tflite_graph_tf2.py`を実行し、TFLite形式に適した中間ファイルを生成します。\n",
        "\n",
        "この中間ファイルがTensorFlow Lite Converterに渡され、最終的にTFLiteモデルが生成されることになります。\n",
        "\n",
        "このプロセスについての詳細は[こちら](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md)を参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyrqHSQQ7WKE"
      },
      "source": [
        "%%bash\n",
        "python models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "  --pipeline_config_path output/pipeline.config \\\n",
        "  --trained_checkpoint_dir output/checkpoint \\\n",
        "  --output_directory tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXuK3pI8KLHL"
      },
      "source": [
        "## TFliteモデルのダウンロード\n",
        "\n",
        "TensorFlow Lite Converterを使用してTFLite形式のモデル(model.tflite)を生成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5hjPyR78bgs"
      },
      "source": [
        "!tflite_convert --saved_model_dir=tflite/saved_model --output_file=tflite/model.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHlXL1x_Z3tc"
      },
      "source": [
        "## TFLiteモデルをテストする\n",
        "\n",
        "出来上がったTFLiteモデルをロードし、テストしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-GZ0fsbTW6x"
      },
      "source": [
        "test_image_dir = 'models/research/object_detection/test_images/ducky/test/'\n",
        "test_images_np = []\n",
        "for i in range(1, 50):\n",
        "  image_path = os.path.join(test_image_dir, 'out' + str(i) + '.jpg')\n",
        "  test_images_np.append(np.expand_dims(\n",
        "      load_image_into_numpy_array(image_path), axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd9dm9DnTdno"
      },
      "source": [
        "def detect(interpreter, input_tensor):\n",
        "  \"\"\"Run detection on an input image.\n",
        "\n",
        "  Args:\n",
        "    interpreter: tf.lite.Interpreter\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "  Returns:\n",
        "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`).\n",
        "  \"\"\"\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  # TFLiteモデルには前処理が含まれていないので、前処理部分は元のモデルを使用しています。\n",
        "  preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "  interpreter.set_tensor(input_details[0]['index'], preprocessed_image.numpy())\n",
        "\n",
        "  interpreter.invoke()\n",
        "\n",
        "  boxes = interpreter.get_tensor(output_details[1]['index'])\n",
        "  classes = interpreter.get_tensor(output_details[3]['index'])\n",
        "  scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "  return boxes, classes, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcE6OwrHQJya"
      },
      "source": [
        "# TFLiteモデルをロードし、メモリ上に展開\n",
        "interpreter = tf.lite.Interpreter(model_path=\"tflite/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# なお、最初の1フレームはtf.functionのトレースが行われて時間がかかりますが、その後の推論は速くなるはずです。\n",
        "\n",
        "label_id_offset = 1\n",
        "for i in range(len(test_images_np)):\n",
        "  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "  boxes, classes, scores = detect(interpreter, input_tensor)\n",
        "\n",
        "  plot_detections(\n",
        "      test_images_np[i][0],\n",
        "      boxes[0],\n",
        "      classes[0].astype(np.uint32) + label_id_offset,\n",
        "      scores[0],\n",
        "      category_index, figsize=(15, 20), image_name=\"gif_frame_\" + ('%02d' % i) + \".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkMPOSQE0x8C"
      },
      "source": [
        "# 推論した結果を描画したjpgファイルをつなげてGIFファイルとして出力します。\n",
        "imageio.plugins.freeimage.download()\n",
        "\n",
        "anim_file = 'duckies_test.gif'\n",
        "\n",
        "filenames = glob.glob('gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "last = -1\n",
        "images = []\n",
        "for filename in filenames:\n",
        "  image = imageio.imread(filename)\n",
        "  images.append(image)\n",
        "\n",
        "imageio.mimsave(anim_file, images, 'GIF-FI', fps=5)\n",
        "\n",
        "display(IPyImage(open(anim_file, 'rb').read()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzaHWsS58_PQ"
      },
      "source": [
        "## モデルをダウンロードします\n",
        "\n",
        "本手順で生成したTFLiteモデルは、Armadillo-IoT ゲートウェイ G4上でも動作します。\n",
        "以下を実行することで生成したTFLite形式のモデルである、tfliteファイルをダウンロードできます。\n",
        "\n",
        "その後の実機上での実行方法については、Armadillo Base OS開発ガイドを参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ6vac3RAY3j"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('tflite/model.tflite') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}